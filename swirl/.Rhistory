library(swirl)
swirl()
dim(pm0)
head(mp0)
head(pm0)
cnames
# [1] "# RD|Action Code|State Code|County Code|Site ID|Parameter|POC|Sample Duration|Unit|Method|Date|Start Time|Sample Value|Null Data Code|Sampling Frequency|Monitor Protocol (MP) ID|Qualifier - 1|Qualifier - 2|Qualifier - 3|Qualifier - 4|Qualifier - 5|Qualifier - 6|Qualifier - 7|Qualifier - 8|Qualifier - 9|Qualifier - 10|Alternate Method Detectable Limit|Uncertainty"
cnames <- strsplit(cnames, "|", fixed = TRUE)
cnames
# Nice, but we don't need all these. Assign to names(pm0) the output of a call to the function make.names with cnames[[1]][wcol] as the argument. The variable wcol holds the indices of the 5 columns we selected (from the 28) to use in this lesson, so those are the column names we'll need. As the name suggests, the function "makes syntactically valid names".
names(pm0) <- make.names(cnames[[1]][wcol])
head(mp0)
head(pm0)
x0 <- pm0$Sample.Value
str(x0)
mean(is.na(x0))
names(pm1) <- make.names(cnames[[1]][wcol])
dim(pm1)
x1 <- pm1$Sample.Value
mean(is.na(x1))
summary(x0)
summary(x1)
boxplot(x0, x1)
boxplot(log10(x0), log(x1))
boxplot(log10(x0), log10(x1))
negative <- x1 < 0
sum(negative, na.rm = TRUE)
mean(negative, na.rm = TRUE)
dates <- pm1$Date
str(dates)
dates <- as.Date(as.character(dates), "%Y%m%d")
head(dates)
q()
library(swirl)
swirl()
hist(dates[negative], "month")
swirl()
# Nice, but we don't need all these. Assign to names(pm0) the output of a call to the function make.names with cnames[[1]][wcol] as the argument. The variable wcol holds the indices of the 5 columns we selected (from the 28) to use in this lesson, so those are the column names we'll need. As the name suggests, the function "makes syntactically valid names".
names(pm0) <- make.names(cnames[[1]][wcol])
# Slides for this and other Data Science courses may be found at github https://github.com/DataScienceSpecialization/courses/. If you care to use them, they must be downloaded as a zip file and viewed locally. This lesson corresponds to 04_ExploratoryAnalysis/CaseStudy
# In this lesson we'll apply some of the techniques we learned in this course to study air pollution data, specifically particulate matter (we'll call it pm25 sometimes), collected by the U.S. Environmental Protection Agency. This website https://www.health.ny.gov/environmental/indoors/air/pmq_a.htm from New York State offers some basic information on this topic if you're interested.
# Particulate matter (less than 2.5 microns in diameter) is a fancy name for dust, and breathing in dust might pose health hazards to the population. We'll study data from two years, 1999 (when monitoring of particulate matter started) and 2012. Our goal is to see if there's been a noticeable decline in this type of air pollution between these two years.
# We've read in 2 large zipped files for you using the R command read.table (which is smart enough to unzip the files).  We stored the 1999 data in the array pm0 for you.
> dim(pm0)
dim(pm0)
head(pm0)
cnames
e
strsplit(cnames, "|", fixed = TRUE)
cnames
# [1] "# RD|Action Code|State Code|County Code|Site ID|Parameter|POC|Sample Duration|Unit|Method|Date|Start Time|Sample Value|Null Data Code|Sampling Frequency|Monitor Protocol (MP) ID|Qualifier - 1|Qualifier - 2|Qualifier - 3|Qualifier - 4|Qualifier - 5|Qualifier - 6|Qualifier - 7|Qualifier - 8|Qualifier - 9|Qualifier - 10|Alternate Method Detectable Limit|Uncertainty"
cnames <- strsplit(cnames, "|", fixed = TRUE)
# Nice, but we don't need all these. Assign to names(pm0) the output of a call to the function make.names with cnames[[1]][wcol] as the argument. The variable wcol holds the indices of the 5 columns we selected (from the 28) to use in this lesson, so those are the column names we'll need. As the name suggests, the function "makes syntactically valid names".
names(pm0) <- make.names(cnames[[1]][wcol])
head(pm0)
> x0 <- pm0$Sample.Value
x0 <- pm0$Sample.Value
str(x0)
names(pm1) <- make.names(cnames[[1]][wcol])
x1 <- pm1$Sample.Value
negative <- x1 < 0
dates <- pm1$Date
dates <- as.Date(as.character(dates), "%Y%m%d")
swirl()
cnames
# [1] "# RD|Action Code|State Code|County Code|Site ID|Parameter|POC|Sample Duration|Unit|Method|Date|Start Time|Sample Value|Null Data Code|Sampling Frequency|Monitor Protocol (MP) ID|Qualifier - 1|Qualifier - 2|Qualifier - 3|Qualifier - 4|Qualifier - 5|Qualifier - 6|Qualifier - 7|Qualifier - 8|Qualifier - 9|Qualifier - 10|Alternate Method Detectable Limit|Uncertainty"
cnames <- strsplit(cnames, "|", fixed = TRUE)
cnames
# Nice, but we don't need all these. Assign to names(pm0) the output of a call to the function make.names with cnames[[1]][wcol] as the argument. The variable wcol holds the indices of the 5 columns we selected (from the 28) to use in this lesson, so those are the column names we'll need. As the name suggests, the function "makes syntactically valid names".
names(pm0) <- make.names(cnames[[1]][wcol])
swirl()
swirl()
swirl()
dim(pm0)
head(pm0)
cnames
cnames <- strsplit(cnames, "|", fixed = TRUE)
cnames
head(pm0)
names(pm0) <- make.names(cnames[[1]][wcol])
head(pm0)
x0 <- pm0$Sample.Value
str(x0)
mean(is.na(x0))
names(pm1) <- make.names(cnames[[1]][wcol])
dim(pm1)
x1 <- pm1$Sample.Value
mean(is.na(x1))
summary(x0)
summary(x1)
boxplot(x0, x1)
oxplot(x0, x1)
boxplot(x0, x1)
boxplot(log10(x0), log10(x1))
negative <- x1 < 0
sum(negative, na.rm = TRUE)
mean(negative, na.rm = TRUE)
dates <- pm1$Date
str(dates)
dates <- as.Date(as.character(dates), "%Y%m%d")
head(dates)
hist(dates[negative], "month")
str(site0)
intersect(site0, site1)
both <- intersect(site0, site1)
both
head(pm0)
cnt0 <- subset(pm0, State.Code == 36 & contry.site %in% both)
cnt0 <- subset(pm0, State.Code == 36 & conty.site %in% both)
cnt0 <- subset(pm0, State.Code == 36 & county.site %in% both)
cnt1 <- subset(pm1, State.Code == 36 & county.site %in% both)
sapply(split(cnt0, cnt0$county.site), nrow)
sapply(split(cnt1, cnt1$county.site), nrow)
pm0sub <- subset(cnt0$County.Code == 63 & cnt0$Site.ID == 2008)
pm0sub <- subset(cnt0, County.Code == 63 & Site.ID == 2008)
pm1sub <- subset(cnt1, County.Code == 63 & Site.ID == 2008)
x0sub <- pm0sub$Sample.Value
x1sub <- pm1sub$Sample.Value
dates0 <- as.Date(as.character(pm0$Date), "%Y%m%d")
dates0 <- as.Date(as.character(pm0sub$Date), "%Y%m%d")
dates1 <- as.Date(as.character(pm1sub$Date), "%Y%m%d")
par(mfrow = c(1,2), mar = c(4,4,2,1))
plot(dates0, x0sub, pch = 20)
abline(h = median(x0sub, na.rm=TRUE), lwd = 2)
plot(dates1, x1sub, pch = 20)
abline(h = median(x1sub, na.rm=TRUE), lwd = 2)
rng <- range(x0sub, s1sub, na.rm = TRUE)
rng <- range(x0sub, x1sub, na.rm = TRUE)
rng
# Let's first gather the mean (average measurement) for each state in 1999. Recall that the original data for this year was stored in pm0.
mn0 <- with(pm0, tapply(Sample.Value, State.Code, mean, na.rm = TRUE))
str(mn0)
mn1 <- with(pm1, tapply(Sample.Value, State.Code, mean, na.rm = TRUE))
str(mn1)
summary(mn0)
summary(mn1)
d0 <- data.frame(state = names(mn0), mean = mn0)
d1 <- data.frame(state = names(mn1), mean = mn1)
merge(d0, d1, by = "state")
mrg <- merge(d0, d1, by = "state")
dim(mrg)
head(mrg)
with(mrg, plot(rep(1,52), mrg[,2], xlim = c(.5, 2.5)))
with(mrg, points(x = rep(2,52, y = mrg[,3])))
with(mrg, points(rep(2, 52), mrg[, 3]))
a
segments(with(mrg, points(rep(2, 52), mrg[, 3])))
with(mrg, segments(rep(1,52), mrg[,2], rep(2,52), mrg[,3]))
segments(rep(1, 52), mrg[, 2], rep(2, 52), mrg[, 3])
mrg[mrg$mean.x <- mrg$mean.y, ]
mrg[mrg$mean.x < mrg$mean.y, ]
q()
library(swirl)
swirl()
11/12
deck
52
4/52
0
12/52
1/51
2/51
swirl()
swirl()
(1.6*.8)/2
0.64/2
0.64
mypdf
integrate(mypdf, lower = 0, upper = 1.6)
5
2
.5
0.8
.4
info()
l
8
equiv_val(sqrt(2))
sqrt(2)
swirl()
swirl()
q()
library(swirl())
swirl()
swirl()
swirl()
(.8*1.6)/2
.64
mypdf
integrate(mypdf, lower = 0, upper = 0.64)
# Now use the R function integrate to integrate mypdf with the parameters lower equal to 0 and upper equal to 1.6. See if you get the same area (probability) you got before.
> integrate(mypdf, lower = 0, upper = 1.6)
integrate(mypdf, lower = 0, upper = 1.6)
sqrt(2)
swirl()
swirl()
swirl()
library(swirl)
swirl()
swirl
swirl()
99.7*0.001
0.997*0.001
.015*.999
99.7*0.001 / ( 99.7*0.001 + 015*.999)
.997*0.001 / ( 99.7*0.001 + 015*.999)
.997*0.001 / ( .997*0.001 + 015*.999)
.997*0.001 / ( .997*0.001 + .015*.999)
swirl()
sum(1:6)/6
expect_dice
dice_high
expect_dice(dice_high)
expect_dice(dice_low)
expect_dice((edh+edl/2))
0.5 * (edh + edl)
swirl()
integrate(myfunc, lower = 0, upper = 2)
myfunc
spop
mean(spop)
allsam
apply(allsam, mean, 1)
apply(allsam, 1, mean)
mean(smeans)
swirl()
swirl()
dice_sqr
ex2_fair <- sum(dice_sqr * dice_fair)
ex2_fair - 3.5^2
sum(dice_sqr * dice_high) - 3.5^2
sum(dice_high * dice_sqr)-edh^2
sd(apply(matrix(rnorm(10000), 1000), 1, mean))
1/sqrt(10)
1/sqrt(120)
sd(apply(matrix(runif(10000),1000),1,mean))
sd(apply(matrix(poisson(10000),1000),1,mean))
2/sqrt(10)
sd(apply(matrix(rpois(10000,4),1000),1,mean))
1/(2 * sqrt(10))
sd(apply(matrix(sample(0:1,10000,TRUE),1000),1,mean))
choose(5,3)*(.8)^3*(.2)^(5-3) + choose(5,4)*(.8)^4*(.2)^(5-4) + choose(5,5)*(.8)^5*(.2)^(5-5)
pbinom(quantile = 2, size = 5, prob = 0.8, lower.tail = FALSE)
pbinom(2, size = 5, prob = 0.8, lower.tail = FALSE)
q()
library(swirl)
swirl()
qnorm(0.1)
0
qnorm(mean = 3, sd = 2, p = 0.975)
1.96*2+3
pnorm(q = 1200, mean = 1020, sd = 50, lower.tail = FALSE)
pnorm((1200 - 1020) / 50, lower.tail = FALSE)
qnorm(p = 0.75, mean = 1020, sd = 50)
.53
0.53
ppois(q = 3, mean = 2.5 * 4)
ppois(q = 3)
ppois(q = 3, lambda = 2.5 * 4)
pbinom(q = 5, size = 1000, prob = 0.01)
ppois(q = 5, lambda = 1000*0.01)
coinPlot
coinPlot(10)
coinPlot(1000)
coinPlot(10000)
qnorm(p = 0.95)
0.6 + c(-1,1) * qnorm(0.975) * sqrt(0.6 * (1 - 0.6)/100)
binom(60, 100)$conf.int
binom(60, 100)
binom.test(60, 100)$conf.int
mywald
mywald(.2)
ACCompar
ACCompar(20)
lamb <- 5/94.32
lamb +c(-1,1) * qnorm(.975) * sqrt(lamb/94.32)
poisson.test(5, 94.32)$conf
swirl()
swirl()
myplot
myplot(2)
qt(.975, df = 2)
myplot(20)
myplot2
myplot2(2)
qt(.975, df = 2)
myplot2(20)
sleep
range(g1)
range(g2)
difference <- g2 - g1
mean(difference)
mn + c(-1,1) * qt(.975, 9) * s / sqrt(10)
s <- sd(difference)
mn + c(-1,1) * qt(.975, 9) * s / sqrt(10)
t.test(difference)$conf.int
#  The first is a group of 8 oral contraceptive users and the second is a group of 21 controls. The two means are X'_{oc}=132.86 and X'_{c}=127.44, and the two sample standard deviations are s_{oc}= 15.34 and s_{c}= 18.23. Let's first compute the numerator of the pooled sample variance by weighting the sum of the two by their respective sample sizes. Recall the formula (n_x-1)(S_x)^2+(n_y-1)(S_y)^2 and fill in the values to create a variable sp.
sp <- (7 * 15.34^2) + (20 * 18.23^2)
# Now how many degrees of freedom are there? Put your answer in the variable ns.
sp <- sqrt(sp / ns)
# Now how many degrees of freedom are there? Put your answer in the variable ns.
ns <- 8 + 21 - 2
# Now to find the 95% confidence interval. Recall our basic formula X' +/- t_(n-1)*s/sqrt(n) and all the changes we need to make for working with two independent samples. We'll plug in the difference of the sample means for X' and our variable ns for the degrees of freedom when finding the t quantile. For the standard error, we multiply sp by the square root of the sum 1/n_{oc} + 1/n_{c}. The values for this problem are X'_{oc}=132.86 and X'_{c}=127.44, n_{oc}=8 and n_{c}=21. Be sure to use the R construct c(-1,1) for the +/- portion and the R function qt with the correct percentile and degrees of freedom.
132.86 - 127.44 + c(-1,1) * qt(.975, ns) * sp * sqrt(1/8+1/21)
# Now divide sp by ns, take the square root and put the result back in sp.
sp <- sqrt(sp / ns)
# Now to find the 95% confidence interval. Recall our basic formula X' +/- t_(n-1)*s/sqrt(n) and all the changes we need to make for working with two independent samples. We'll plug in the difference of the sample means for X' and our variable ns for the degrees of freedom when finding the t quantile. For the standard error, we multiply sp by the square root of the sum 1/n_{oc} + 1/n_{c}. The values for this problem are X'_{oc}=132.86 and X'_{c}=127.44, n_{oc}=8 and n_{c}=21. Be sure to use the R construct c(-1,1) for the +/- portion and the R function qt with the correct percentile and degrees of freedom.
132.86 - 127.44 + c(-1,1) * qt(.975, ns) * sp * sqrt(1/8+1/21)
# Let's compute the sample pooled variance and store it in the variable sp. Recall that this is the sqrt(weighted sums of sample variances/deg of freedom). The weight of each is the sample size-1. Use the R function var to compute the variances of g1 and g2. The degrees of freedom is 10+10-2 = 18.
sp <- sqrt((9*var(g1)+9*var(g2))/18)
# Now the last term of the formula, the standard error of the mean difference, is simply sp times the square root of the sum 1/10 + 1/10. Find the 95% t confidence interval of the mean difference of the two groups g1 and g2. Substitute md and sp into the formula you used above.
md + c(-1,1)*qt(.975,18)*sp*sqrt(1/5)
# We can check this manual calculation against the R function t.test. Since we subtracted g1 from g2, be sure to place g2 as your first argument and g1 as your second. Also make sure the argument paired is FALSE and var.equal is TRUE. We only need the confidence interval so use the construct x$conf.  Do this now.
t.test(g2, g1, paired = FALSE, var.equal = TRUE)$conf
# Pretty cool that it matches, right? Note that 0 is again in this 95% interval so you can't reject the claim that the two groups are the same. (Recall that this is the opposite of what we saw with paired data.) Let's run t.test again, this time with paired=TRUE and see how different the result is. Don't specify var.equal and look only at the confidence interval.
t.test(g2, g1, paired = TRUE)$conf
# Just as we saw when we ran t.test on our vector, difference! See how the interval excludes 0? This means the groups when paired have much different averages.
# Now let's talk about calculating confidence intervals for two groups which have unequal variances. We won't be pooling them as we did before.
# In this case the formula for the interval is similar to what we saw before, Y'-X' +/- t_df * SE, where as before Y'-X' represents the difference of the sample means. However, the standard error SE and the quantile t_df are calculated differently from previous methods. Here SE is the square root of the sum of the squared standard errors of the two means, (s_1)^2/n_1 + (s_2)^2/n_2 .
# When the underlying X and Y data are iid normal and the variances are different, the normalized statistic we started this lesson with, (X'-mu)/(s/sqrt(n)), doesn't follow a t distribution. However, it can be approximated by a t distribution if we set the degrees of freedom appropriately.
# The formula for the degrees of freedom is a complicated fraction that no one remembers.  The numerator is the SQUARE of the sum of the squared standard errors of the two sample means. Each has the form s^2/n. The denominator is the sum of two terms, one for each group. Each term has the same form. It is the standard error of the mean raised to the fourth power divided by the sample size-1. More precisely, each term looks like (s^4/n^2)/(n-1). We use this df to find the t quantile.
# Here's the formula. You might have to stretch the plot window to get it displayed more clearly.
# Let's plug in the numbers from the blood pressure study to see how this works. Recall we have two groups, the first with size 8 and X'_{oc}=132.86 and s_{oc}=15.34 and the second with size 21 and X'_{c}=127.44 and s_{c}=18.23.
# Let's compute the degrees of freedom first. Start with the numerator. It's the square of the sum of two terms. Each term is of the form s^2/n. Do this now and put the result in num. Our numbers were 15.34 with size 8 and 18.23 with size 21.
num <- (15.34^2/8 + 18.23^2/21)^2
# Now the denominator. This is the sum of two terms. Each term has the form s^4/n^2/(n-1). These look a little different than the form displayed but they're equivalent. Put the result in the variable den. Our numbers were 15.34 with size 8 and 18.23 with size 21.
den <- 15.34^4/8^2/7 + 18.23^4/21^2/20
# Now divide num by den and put the result in mydf.
mydf <- num / den
# Now with the R function qt(.975,mydf) compute the 95% t interval. Recall the formula. X'_{oc}-X'_{c} +/- t_df * SE. Recall that SE is the square root of the sum of the squared standard errors of the two means, (s_1)^2/n_1 + (s_2)^2/n_2 . Again our numbers are the following. X'_{oc}=132.86 s_{oc}=15.34 and n_{oc}=8 . X'_{c}=127.44 s_{c}=18.23 and n_{c}=21.
132.86-127.44 +c(-1,1)*qt(.975,mydf)*sqrt(15.34^2/8 + 18.23^2/21)
10 / sqrt(100)
32-30/1
(32-30)/(10/4)
15
qt(.95, 15)
# Look at the dimensions of fs now using the R function dim.
dim(fs)
# So fs has 1078 rows and 2 columns. The columns, fheight and sheight, contain the heights of a father and his son. Obviously there are 1078 such pairs. We can run t.test on this data in one of two ways. First, we can run it with just one argument, the difference between the heights, say fs$sheight-fs$fheight. OR we can run it with three arguments, the two heights plus the paired argument set to TRUE. Run t.test now using whichever way you prefer.
t.test(fs$sheight, fs$fheight, paired = TRUE)
11.7885 * sd(fs$sheight-fs$fheight)/sqrt(1078)
mybin
8
swirl()
swirl()
pt(q = 2.5, df = 15, lower.tail = FALSE)
qnorm(.95)
qnorm(.99)
pnorm(q = 2, lower.tail = TRUE)
pnorm(q = 2, lower.tail = FALSE)
mybin
pbinom(q = 6, size = 8, prob = .5, lower.tail = FALSE)
pbinom(q = 7, size = 8, prob = .5, lower.tail = TRUE)
ppois(q = 9, lambda = 5, lower.tail = FALSE)
mypdf(34)
myplot(34)
myplot(33.3)
myplot(30)
myplot(28)
z <- qnorm(.95)
pnorm(q = 30 + z, mean = 30, lower.tail = FALSE)
pnorm(q = 30 + z, mean = 32, lower.tail = FALSE)
pnorm(q = 30 + z, mean = 32, sd = 1, lower.tail = FALSE)
pnorm(q = 30 + z * 2, mean = 32, sd = 2, lower.tail = FALSE)
power.t.test(n = 16, delta = 2 / 4, sd = 1, type = "one.sample", alt = "one.sided")$power
power.t.test(n = 16, delta = 2, sd = 4, type = "one.sample", alt = "one.sided")$power
power.t.test(n = 16, delta = 100, sd = 200, type = "one.sample", alt = "one.sided")$power
power.t.test(power = .8, delta = 2 / 4, sd = 1, type = "one.sample", alt = "one.sided")$n
power.t.test(power = .8, delta = 2, sd = 4, type = "one.sample", alt = "one.sided")$n
power.t.test(power = .8, delta = 100, sd = 200, type = "one.sample", alt = "one.sided")$n
power.t.test(power = .8, n = 26, sd = 1, type = "one.sample", alt = "one.sided")$delta
power.t.test(power = .8, n = 27, sd = 1, type = "one.sample", alt = "one.sided")$delta
swirl()
swirl()
swirl()
q()
